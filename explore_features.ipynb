{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4b53d7",
   "metadata": {},
   "source": [
    "# Feature Exploration Notebook\n",
    "\n",
    "This notebook is for exploring the features generated by `build_features.py`. We will:\n",
    "1.  Load the saved preprocessing transformers (`TfidfVectorizer`s and `OneHotEncoder`).\n",
    "2.  Inspect the vocabularies of the N-gram vectorizers.\n",
    "3.  Examine how a sample antibody sequence is transformed into a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca722050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded all transformers.\n",
      "VH Vocabulary Size: 10000\n",
      "VL Vocabulary Size: 10000\n",
      "OHE Categories: [array(['IgG1', 'IgG2', 'IgG4'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Add the 'src' directory to the Python path to allow importing the config module\n",
    "# This assumes the notebook is in the root of the project directory\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append('src')\n",
    "\n",
    "import config\n",
    "\n",
    "# --- Load Artefacts ---\n",
    "# Load the three fitted transformers we created in the build_features script.\n",
    "try:\n",
    "    vectorizer_vh = joblib.load(config.ARTEFACTS_DIR / \"vectorizer_vh.joblib\")\n",
    "    vectorizer_vl = joblib.load(config.ARTEFACTS_DIR / \"vectorizer_vl.joblib\")\n",
    "    encoder_ohe = joblib.load(config.ARTEFACTS_DIR / \"encoder_ohe.joblib\")\n",
    "    print(\"Successfully loaded all transformers.\")\n",
    "    print(f\"VH Vocabulary Size: {len(vectorizer_vh.vocabulary_)}\")\n",
    "    print(f\"VL Vocabulary Size: {len(vectorizer_vl.vocabulary_)}\")\n",
    "    print(f\"OHE Categories: {encoder_ohe.categories_}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading transformers: {e}\")\n",
    "    print(\"Please run the `src/features/build_features.py` script first to generate the artefacts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4838bd9",
   "metadata": {},
   "source": [
    "## 1. Inspecting N-gram Vocabularies\n",
    "\n",
    "Let's examine the N-grams that were selected for the vocabularies. We can look at the N-grams with the highest and lowest IDF (Inverse Document Frequency) scores.\n",
    "\n",
    "*   **High IDF**: N-grams that are rare across all sequences. These are highly specific.\n",
    "*   **Low IDF**: N-grams that are very common across all sequences.\n",
    "\n",
    "We'll create a helper function to display the top and bottom N-grams by their IDF weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa98a710",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minspect_vocabulary\u001b[39m(vectorizer: \u001b[43mTfidfVectorizer\u001b[49m, top_n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Displays the N-grams with the highest and lowest IDF scores from a fitted TfidfVectorizer.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Get the feature names (n-grams) and their IDF scores\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "def inspect_vocabulary(vectorizer: TfidfVectorizer, top_n: int = 20):\n",
    "    \"\"\"\n",
    "    Displays the N-grams with the highest and lowest IDF scores from a fitted TfidfVectorizer.\n",
    "    \"\"\"\n",
    "    # Get the feature names (n-grams) and their IDF scores\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    idf_scores = vectorizer.idf_\n",
    "\n",
    "    # Create a DataFrame for easy sorting\n",
    "    idf_df = pd.DataFrame({\n",
    "        'ngram': feature_names,\n",
    "        'idf_score': idf_scores\n",
    "    }).sort_values(by='idf_score', ascending=False)\n",
    "\n",
    "    print(f\"--- Top {top_n} N-grams (Most Specific/Rare) ---\")\n",
    "    print(idf_df.head(top_n).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n--- Bottom {top_n} N-grams (Most Common) ---\")\n",
    "    print(idf_df.tail(top_n).to_string(index=False))\n",
    "\n",
    "# Inspect the VH vocabulary\n",
    "print(\"=\"*20 + \" VH Vocabulary \" + \"=\"*20)\n",
    "inspect_vocabulary(vectorizer_vh)\n",
    "\n",
    "# Inspect the VL vocabulary\n",
    "print(\"\\n\" + \"=\"*20 + \" VL Vocabulary \" + \"=\"*20)\n",
    "inspect_vocabulary(vectorizer_vl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f265f",
   "metadata": {},
   "source": [
    "## 2. Vectorizing a Sample Sequence\n",
    "\n",
    "Now, let's see how a single antibody's heavy and light chains are converted into sparse vectors. We will:\n",
    "1.  Load the original dataset to get a sample sequence.\n",
    "2.  Use the `.transform()` method of our fitted vectorizers.\n",
    "3.  Inspect the resulting sparse matrix to see which N-grams were found and what their TF-IDF scores are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_vectorized_sequence(sequence: str, vectorizer: TfidfVectorizer):\n",
    "    \"\"\"\n",
    "    Transforms a sequence and displays the non-zero elements of its vector.\n",
    "    \"\"\"\n",
    "    # Transform the sequence. Note that transform expects an iterable.\n",
    "    vector = vectorizer.transform([sequence])\n",
    "    \n",
    "    # Get feature names and find non-zero elements\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    non_zero_indices = vector.nonzero()[1]\n",
    "    \n",
    "    if len(non_zero_indices) == 0:\n",
    "        print(\"No N-grams from the vocabulary were found in this sequence.\")\n",
    "        return\n",
    "\n",
    "    # Create a DataFrame of the results\n",
    "    results = []\n",
    "    for idx in non_zero_indices:\n",
    "        ngram = feature_names[idx]\n",
    "        tfidf_score = vector[0, idx]\n",
    "        results.append({'ngram': ngram, 'tfidf_score': tfidf_score})\n",
    "        \n",
    "    result_df = pd.DataFrame(results).sort_values(by='tfidf_score', ascending=False)\n",
    "    \n",
    "    print(f\"Original Sequence (first 60 chars): '{sequence[:60]}...'\")\n",
    "    print(f\"Vector Shape: {vector.shape}\")\n",
    "    print(f\"Number of non-zero N-grams found: {len(result_df)}\")\n",
    "    print(\"\\n--- Found N-grams and their TF-IDF Scores ---\")\n",
    "    print(result_df.to_string(index=False))\n",
    "\n",
    "# --- Load data and get a sample ---\n",
    "try:\n",
    "    df = pd.read_csv(config.DATA_FILE)\n",
    "    # Take a sample from the middle of the dataframe\n",
    "    sample_antibody = df.iloc[len(df) // 2]\n",
    "\n",
    "    vh_sequence = sample_antibody[config.VH_SEQUENCE_COL]\n",
    "    vl_sequence = sample_antibody[config.VL_SEQUENCE_COL]\n",
    "    subtype = sample_antibody[config.HC_SUBTYPE_COL]\n",
    "\n",
    "    # --- Vectorize and display VH sequence ---\n",
    "    print(\"=\"*20 + \" Vectorizing VH Sequence \" + \"=\"*20)\n",
    "    display_vectorized_sequence(vh_sequence, vectorizer_vh)\n",
    "\n",
    "    # --- Vectorize and display VL sequence ---\n",
    "    print(\"\\n\" + \"=\"*20 + \" Vectorizing VL Sequence \" + \"=\"*20)\n",
    "    display_vectorized_sequence(vl_sequence, vectorizer_vl)\n",
    "    \n",
    "    # --- Transform the subtype ---\n",
    "    print(\"\\n\" + \"=\"*20 + \" Encoding Subtype \" + \"=\"*20)\n",
    "    print(f\"Original Subtype: {subtype}\")\n",
    "    subtype_vector = encoder_ohe.transform([[subtype]])\n",
    "    print(f\"Encoded Vector (sparse): {subtype_vector}\")\n",
    "    print(f\"Feature names: {encoder_ohe.get_feature_names_out()}\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Could not load data file at {config.DATA_FILE} to get a sample.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strawman-ab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
